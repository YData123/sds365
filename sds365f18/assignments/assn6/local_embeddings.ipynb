{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "import sklearn.metrics.pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute unigram frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.63321876525879\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "fdist = Counter()\n",
    "for rec in open ('/data/text8', 'r'):\n",
    "    rec = rec.strip()\n",
    "    fdist.update(word_tokenize(rec))\n",
    "print(time.time()-s)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct vocabulary: remove stopwords and unigrams with frequency less than UNIGRAM_LB. You should aim for around 5000 or fewer words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253849\n",
      "5044\n"
     ]
    }
   ],
   "source": [
    "UNIGRAM_LB = 300\n",
    "sw         = stopwords.words(\"english\")\n",
    "vocab      = sorted([v for v in fdist.keys() if fdist[v] > UNIGRAM_LB and len(v) >= 3 and v not in sw])\n",
    "vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "print(len(fdist))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prune text by removed words outside of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_word = ''\n",
    "pruned_text  = []\n",
    "proc = open('pruned-text8.txt', 'w')\n",
    "with open('/data/text8', 'r') as f:\n",
    "    for rec in f:\n",
    "        rec = rec.strip()\n",
    "        pruned_text_list = [w for w in rec.split() if w in vocab_dict]\n",
    "        rec = ' '.join(pruned_text_list)\n",
    "        pruned_text.append(pruned_text_list)\n",
    "        proc.write(rec + '\\n')\n",
    "pruned_text = list(itertools.chain.from_iterable(pruned_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute cooccurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "88.88820195198059\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 5\n",
    "#co           = np.zeros((len(vocab), len(vocab)))\n",
    "s            = time.time()\n",
    "co_s         = sp.sparse.csr_matrix((len(vocab), len(vocab))) # empty matrix\n",
    "\n",
    "# for sparse matrix construction\n",
    "d    = []\n",
    "rows = []\n",
    "cols = []\n",
    "\n",
    "for h_idx in range(len(pruned_text)):\n",
    "    \n",
    "    l_idx = max(0, h_idx - CONTEXT_SIZE)\n",
    "    r_idx = min(len(pruned_text)-1, h_idx + CONTEXT_SIZE)\n",
    "\n",
    "    for l in range(l_idx, h_idx):\n",
    "        #co[vocab_dict[pruned_text[h_idx]], vocab_dict[pruned_text[l]]] += 1\n",
    "        rows.append(vocab_dict[pruned_text[h_idx]])\n",
    "        cols.append(vocab_dict[pruned_text[l]])\n",
    "        d.append(1)\n",
    "        \n",
    "    for r in range(h_idx, r_idx+1):\n",
    "        #co[vocab_dict[pruned_text[h_idx]], vocab_dict[pruned_text[r]]] += 1\n",
    "        rows.append(vocab_dict[pruned_text[h_idx]])\n",
    "        cols.append(vocab_dict[pruned_text[r]])\n",
    "        d.append(1)\n",
    "    \n",
    "    if h_idx % 1000000 == 0 and h_idx != 0:\n",
    "        print(h_idx)\n",
    "        co_s = co_s + sp.sparse.csr_matrix((d, (rows, cols)), shape=(len(vocab), len(vocab)))\n",
    "        d    = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "if len(d) > 0:\n",
    "    co_s = co_s + sp.sparse.csr_matrix((d, (rows, cols)), shape=(len(vocab), len(vocab)))\n",
    "\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute k-SVD of coccurence matrix and pairwise distances between resulting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = scipy.sparse.linalg.svds(co_s, k=50)\n",
    "# compute pairwise distances between each embedding vector\n",
    "D = sklearn.metrics.pairwise.pairwise_distances(V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(V.T[0])\n",
    "del co_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find nearest neighbors for some example words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "republican: ['senate', 'representatives', 'legislation', 'presidential']\n",
      "physics: ['chemistry', 'mathematical', 'psychology', 'biology']\n",
      "baseball: ['sports', 'professional', 'teams', 'jazz']\n",
      "chicago: ['colleges', 'campus', 'universities', 'texas']\n",
      "fish: ['plants', 'hot', 'extreme', 'wild']\n",
      "algebra: ['vector', 'theorem', 'linear', 'binary']\n",
      "rock: ['musical', 'song', 'playing', 'folk']\n",
      "food: ['animals', 'effects', 'products', 'variety']\n",
      "einstein: ['explained', 'creative', 'experiment', 'maxwell']\n"
     ]
    }
   ],
   "source": [
    "test_words = ['republican', 'physics', 'baseball', 'chicago', 'fish', 'algebra', 'rock', 'food', 'einstein']\n",
    "for t in test_words:\n",
    "    print(\"{}: {}\".format(t, [vocab[np.argsort(D[vocab_dict[t],:])[i]] for i in range(1,5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you will need to recompute this by transforming the counts to the appropriate PMI values. It will also be necessary to put the resulting vectors into a form that can be read by gensim."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
