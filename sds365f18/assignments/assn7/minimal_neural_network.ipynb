{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetID: !! ADD NET_ID !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal neural network implementation\n",
    "\n",
    "This is a \"bare bones\" implementation of a 2-layer neural network for classification, using rectified linear units as activation functions. The code is from Andrej Karpathy; please see [this page](http://cs231n.github.io/neural-networks-case-study/) for an annotated description of the code.\n",
    "\n",
    "Your task in this part of the assigment is to extend this to a 3-layer network, and to experiment with some different settings of the parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['axes.facecolor'] = 'lightgray'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.3 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim([-1.5,1.5])\n",
    "plt.ylim([-1.5,1.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_linear_classifier(lambda_param=1e-3):\n",
    "\n",
    "    # initialize parameters randomly\n",
    "    W = 0.01 * np.random.randn(D,K)\n",
    "    b = np.zeros((1,K))\n",
    "\n",
    "    # some hyperparameters\n",
    "    step_size = 1e-0\n",
    "\n",
    "    # gradient descent loop\n",
    "    num_examples = X.shape[0]\n",
    "    for i in range(200):\n",
    "  \n",
    "      # evaluate class scores, [N x K]\n",
    "      scores = np.dot(X, W) + b \n",
    "  \n",
    "      # compute the class probabilities\n",
    "      exp_scores = np.exp(scores)\n",
    "      probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "      # compute the loss: average cross-entropy loss and regularization\n",
    "      correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "      data_loss = np.sum(correct_logprobs)/num_examples\n",
    "      reg_loss = 0.5*lambda_param*np.sum(W*W)\n",
    "      loss = data_loss + reg_loss\n",
    "      if i % 10 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "  \n",
    "      # compute the gradient on scores\n",
    "      dscores = np.array(probs)\n",
    "      dscores[range(num_examples),y] -= 1\n",
    "      dscores /= num_examples\n",
    "  \n",
    "      # backpropate the gradient to the parameters (W,b)\n",
    "      dW = np.dot(X.T, dscores)\n",
    "      db = np.sum(dscores, axis=0, keepdims=True)\n",
    "  \n",
    "      dW += lambda_param*W # regularization gradient\n",
    "  \n",
    "      # perform a parameter update\n",
    "      W += -step_size * dW\n",
    "      b += -step_size * db\n",
    "    \n",
    "    return W, b\n",
    "\n",
    "W, b = train_linear_classifier(lambda_param=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate training set accuracy\n",
    "scores = np.dot(X, W) + b\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting classifier\n",
    "h = 0.015\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2_layer_network(H1=100, lambda_param=1e-03):\n",
    "    # initialize parameters randomly\n",
    "    # H1 = 100 # size of hidden layer\n",
    "    W1 = 0.01 * np.random.randn(D,H1)\n",
    "    b1 = np.zeros((1,H1))\n",
    "    W2 = 0.01 * np.random.randn(H1,K)\n",
    "    b2 = np.zeros((1,K))\n",
    "\n",
    "    # some hyperparameters\n",
    "    step_size = 1e-1\n",
    "\n",
    "    # gradient descent loop\n",
    "    num_examples = X.shape[0]\n",
    "    for i in range(20000):\n",
    "  \n",
    "      # evaluate class scores, [N x K]\n",
    "      hidden_layer = np.maximum(0, np.dot(X, W1) + b1) # note, ReLU activation\n",
    "      scores = np.dot(hidden_layer, W2) + b2\n",
    "  \n",
    "      # compute the class probabilities\n",
    "      exp_scores = np.exp(scores)\n",
    "      probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "      # compute the loss: average cross-entropy loss and regularization\n",
    "      correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "      data_loss = np.sum(correct_logprobs)/num_examples\n",
    "      reg_loss = 0.5*lambda_param*np.sum(W1*W1) + 0.5*lambda_param*np.sum(W2*W2)\n",
    "      loss = data_loss + reg_loss\n",
    "      if i % 1000 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "  \n",
    "      # compute the gradient on scores\n",
    "      dscores = np.array(probs)\n",
    "      dscores[range(num_examples),y] -= 1\n",
    "      dscores /= num_examples\n",
    "  \n",
    "      # backpropate the gradient to the parameters\n",
    "      # first backprop into parameters W2 and b2\n",
    "      dW2 = np.dot(hidden_layer.T, dscores)\n",
    "      db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "      # next backprop into hidden layer\n",
    "      dhidden = np.dot(dscores, W2.T)\n",
    "      # backprop the ReLU non-linearity\n",
    "      dhidden[hidden_layer <= 0] = 0\n",
    "      # finally into W,b\n",
    "      dW1 = np.dot(X.T, dhidden)\n",
    "      db1 = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "      # add regularization gradient contribution\n",
    "      dW2 += lambda_param * W2\n",
    "      dW1 += lambda_param * W1\n",
    "  \n",
    "      # perform a parameter update\n",
    "      W1 += -step_size * dW1\n",
    "      b1 += -step_size * db1\n",
    "      W2 += -step_size * dW2\n",
    "      b2 += -step_size * db2\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "W1, b1, W2, b2 = train_2_layer_network(100, lambda_param=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate training set accuracy\n",
    "hidden_layer = np.maximum(0, np.dot(X, W1) + b1)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the resulting classifier\n",
    "h = 0.015\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], W1) + b1), W2) + b2\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Extend the code from two layers to three layers (15 points)\n",
    "\n",
    "Run the code provided in the notebook minimal neural network.ipynb and inspect it to be sure you understand how it works. (We did this in class!) Then, after working out the derivatives in part (a) above, extend the code by writing a function that implements a 3-layer version. Your function declaration should look like this:\n",
    "```python\n",
    "def train_3_layer_network(H1=100, H2=100, lambda_param=1e-03)\n",
    "```    \n",
    "where H1 is the number of hidden units in the first layer, and H2 is the number of hidden units in the second layer. Then train a 3-layer network and display the classification results in your notebook, as is done for the 2-layer network in the starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_3_layer_network(H1=100, H2=100, lambda_param=1e-03):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra credit (7 points)\n",
    "\n",
    "As extra credit, make the code more modular and write a function to implement a general multilayer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_layer_network(H=[100, 100], lambda_param=1e-03):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Experiment with different parameter settings (10 points)\n",
    "\n",
    "Now experiment with different network configurations and training parameters. For example, you can train models with different numbers of hidden nodes H1 and H2, and different settings of the regularization parameter lambda param. Train at least three and no more than five networks. For each network, display the decision boundaries on the training data, and include a Markdown cell that describes its behavior relative to the other networks you train. Specifically, comment on how the different settings of the parameters change the bias and variance of the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
